{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT Framework: Agentic Evaluation with LangChain\n",
    "\n",
    "This notebook demonstrates how to properly evaluate agentic AI pipelines using the CERT SDK with LangChain.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**The Context Requirement**: In agentic evaluation, tool outputs serve as the *implicit context* that enables full reliability metrics. Without context, evaluation degrades to Generation Mode with only weak quality signals.\n",
    "\n",
    "| Evaluation Mode | Context Source | Available Metrics |\n",
    "|-----------------|----------------|-------------------|\n",
    "| RAG | Explicit context | Semantic similarity, NLI, Grounding, **SGI** |\n",
    "| Agentic | Tool outputs (auto-extracted) | Same as RAG + tool integration |\n",
    "| Generation | None | Instruction adherence, self-consistency, format |\n",
    "\n",
    "**The CERT SDK automatically extracts context from tool outputs in agentic mode**, enabling computation of SGI (Source-Grounding Index) and other reliability metrics.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install langchain langchain-openai langchain-core -q\n",
    "\n",
    "# Install CERT SDK (in production, use: pip install cert-sdk[langchain])\n",
    "# For now, install from local\n",
    "!pip install -e ./cert-sdk-main[langchain] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Type\n",
    "\n",
    "# Set API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key-here\"\n",
    "\n",
    "CERT_API_KEY = \"cert_demo_key\"  # Get from dashboard.cert-framework.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Tools\n",
    "\n",
    "We'll create a simple agent with three tools:\n",
    "- **Weather API**: Get current weather\n",
    "- **Calculator**: Perform calculations\n",
    "- **Search**: Search for information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Tool input schemas\n",
    "class WeatherInput(BaseModel):\n",
    "    city: str = Field(description=\"City name to get weather for\")\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    expression: str = Field(description=\"Mathematical expression to evaluate\")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Search query\")\n",
    "\n",
    "\n",
    "@tool(args_schema=WeatherInput)\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Get the current weather for a city.\"\"\"\n",
    "    # Simulated weather data\n",
    "    weather_data = {\n",
    "        \"new york\": {\"temperature\": 72, \"condition\": \"sunny\", \"humidity\": 45},\n",
    "        \"london\": {\"temperature\": 55, \"condition\": \"cloudy\", \"humidity\": 80},\n",
    "        \"tokyo\": {\"temperature\": 68, \"condition\": \"partly cloudy\", \"humidity\": 60},\n",
    "        \"paris\": {\"temperature\": 61, \"condition\": \"rainy\", \"humidity\": 75},\n",
    "    }\n",
    "    city_lower = city.lower()\n",
    "    if city_lower in weather_data:\n",
    "        return {\"city\": city, **weather_data[city_lower]}\n",
    "    return {\"city\": city, \"error\": \"City not found\", \"available_cities\": list(weather_data.keys())}\n",
    "\n",
    "\n",
    "@tool(args_schema=CalculatorInput)\n",
    "def calculator(expression: str) -> dict:\n",
    "    \"\"\"Evaluate a mathematical expression. Supports +, -, *, /, ** operators.\"\"\"\n",
    "    try:\n",
    "        # Safe evaluation of mathematical expressions\n",
    "        allowed_chars = set(\"0123456789+-*/.() \")\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return {\"error\": \"Invalid characters in expression\"}\n",
    "        \n",
    "        result = eval(expression)\n",
    "        return {\"expression\": expression, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"expression\": expression, \"error\": str(e)}\n",
    "\n",
    "\n",
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> dict:\n",
    "    \"\"\"Search for information on a topic.\"\"\"\n",
    "    # Simulated search results\n",
    "    search_db = {\n",
    "        \"python\": [\n",
    "            {\"title\": \"Python Programming\", \"snippet\": \"Python is a high-level programming language known for readability.\"},\n",
    "            {\"title\": \"Python Tutorial\", \"snippet\": \"Learn Python basics including variables, functions, and classes.\"},\n",
    "        ],\n",
    "        \"weather\": [\n",
    "            {\"title\": \"Weather Forecast\", \"snippet\": \"Weather is the state of the atmosphere at a given time and place.\"},\n",
    "            {\"title\": \"Climate vs Weather\", \"snippet\": \"Weather is short-term, climate is long-term average conditions.\"},\n",
    "        ],\n",
    "        \"ai\": [\n",
    "            {\"title\": \"Artificial Intelligence\", \"snippet\": \"AI is the simulation of human intelligence by machines.\"},\n",
    "            {\"title\": \"Machine Learning\", \"snippet\": \"ML is a subset of AI that learns from data.\"},\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    for key, results in search_db.items():\n",
    "        if key in query_lower:\n",
    "            return {\"query\": query, \"results\": results}\n",
    "    \n",
    "    return {\"query\": query, \"results\": [], \"message\": \"No results found\"}\n",
    "\n",
    "\n",
    "# Collect tools\n",
    "tools = [get_weather, calculator, search]\n",
    "print(f\"Defined {len(tools)} tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Agent\n",
    "\n",
    "We'll create a ReAct-style agent using LangChain's tool-calling capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Use gpt-4 for better tool calling\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant with access to various tools.\n",
    "Use the tools when needed to answer questions accurately.\n",
    "Always cite the source of your information (e.g., which tool you used).\"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Create agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,  # CRITICAL: Needed for CERT tracing\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Agent Without CERT\n",
    "\n",
    "First, let's run the agent and see what information is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "query = \"What's the weather in New York? Also, what's 15 * 23?\"\n",
    "\n",
    "# Run agent\n",
    "start_time = time.time()\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULT STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKeys: {result.keys()}\")\n",
    "print(f\"\\nOutput: {result['output']}\")\n",
    "print(f\"\\nDuration: {duration_ms:.0f}ms\")\n",
    "print(f\"\\nIntermediate Steps: {len(result.get('intermediate_steps', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine intermediate steps - these contain tool calls!\n",
    "print(\"\\nINTERMEDIATE STEPS (Tool Calls)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (action, observation) in enumerate(result.get(\"intermediate_steps\", [])):\n",
    "    print(f\"\\n--- Step {i+1} ---\")\n",
    "    print(f\"Tool: {action.tool}\")\n",
    "    print(f\"Input: {action.tool_input}\")\n",
    "    print(f\"Output: {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: Manual Tracing with CERT\n",
    "\n",
    "We can manually construct tool calls and trace them. This gives full control but requires more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CERT SDK\n",
    "import sys\n",
    "sys.path.insert(0, './cert-sdk-main/src')\n",
    "\n",
    "from cert import CertClient, extract_context_from_tool_calls\n",
    "\n",
    "# Initialize client (use mock endpoint for demo)\n",
    "client = CertClient(\n",
    "    api_key=CERT_API_KEY,\n",
    "    dashboard_url=\"http://localhost:3000\",  # Local dev server\n",
    ")\n",
    "\n",
    "print(\"CERT client initialized\")\n",
    "print(f\"Auto-extract context: {client.auto_extract_context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_langchain_result_manually(\n",
    "    client: CertClient,\n",
    "    query: str,\n",
    "    result: dict,\n",
    "    duration_ms: float,\n",
    "    provider: str = \"openai\",\n",
    "    model: str = \"gpt-4\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Manually trace a LangChain agent result.\n",
    "    \n",
    "    This demonstrates the data extraction process:\n",
    "    1. Extract tool calls from intermediate_steps\n",
    "    2. Pass to CERT (context is auto-extracted from tool outputs)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract tool calls from intermediate steps\n",
    "    tool_calls = []\n",
    "    intermediate_steps = result.get(\"intermediate_steps\", [])\n",
    "    \n",
    "    for action, observation in intermediate_steps:\n",
    "        tool_call = {\n",
    "            \"name\": action.tool,\n",
    "            \"input\": action.tool_input if isinstance(action.tool_input, dict) \n",
    "                     else {\"input\": str(action.tool_input)},\n",
    "            \"output\": observation if isinstance(observation, (dict, list, str, int, float))\n",
    "                      else str(observation),\n",
    "        }\n",
    "        tool_calls.append(tool_call)\n",
    "    \n",
    "    # Show what context will be auto-extracted\n",
    "    if tool_calls:\n",
    "        extracted_context = extract_context_from_tool_calls(tool_calls)\n",
    "        print(\"\\nğŸ“‹ AUTO-EXTRACTED CONTEXT:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(extracted_context)\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Send trace to CERT\n",
    "    # Note: context is NOT passed - it will be auto-extracted from tool_calls!\n",
    "    client.trace(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        input_text=query,\n",
    "        output_text=result[\"output\"],\n",
    "        duration_ms=duration_ms,\n",
    "        eval_mode=\"agentic\",  # Or \"auto\" - will detect from tool_calls\n",
    "        tool_calls=tool_calls,\n",
    "        goal_description=query,\n",
    "        metadata={\n",
    "            \"framework\": \"langchain\",\n",
    "            \"tool_count\": len(tool_calls),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Traced with {len(tool_calls)} tool calls\")\n",
    "    print(f\"   Eval mode: agentic\")\n",
    "    print(f\"   Context: auto-extracted ({len(extracted_context) if tool_calls else 0} chars)\")\n",
    "    \n",
    "    return tool_calls\n",
    "\n",
    "\n",
    "# Trace the previous result\n",
    "tool_calls = trace_langchain_result_manually(\n",
    "    client=client,\n",
    "    query=query,\n",
    "    result=result,\n",
    "    duration_ms=duration_ms,\n",
    "    model=\"gpt-4o-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 2: Using CERT LangChain Handler (Recommended)\n",
    "\n",
    "The CERT SDK provides a callback handler that automatically captures everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cert.integrations.langchain import CERTLangChainHandler\n",
    "\n",
    "# Create handler\n",
    "handler = CERTLangChainHandler(\n",
    "    cert_client=client,\n",
    "    default_provider=\"openai\",\n",
    "    default_model=\"gpt-4o-mini\",\n",
    "    auto_flush=True,\n",
    ")\n",
    "\n",
    "print(\"CERT LangChain handler created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent with CERT handler\n",
    "query2 = \"Search for information about AI and tell me what you find.\"\n",
    "\n",
    "result2 = agent_executor.invoke(\n",
    "    {\"input\": query2},\n",
    "    config={\"callbacks\": [handler]}  # Add CERT handler!\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{result2['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use trace_from_result for more control\n",
    "query3 = \"What's 100 divided by 4, plus 50?\"\n",
    "\n",
    "start_time = time.time()\n",
    "result3 = agent_executor.invoke(\n",
    "    {\"input\": query3},\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "duration_ms3 = (time.time() - start_time) * 1000\n",
    "\n",
    "# Trace using helper method\n",
    "handler.trace_from_result(\n",
    "    input_text=query3,\n",
    "    result=result3,\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    duration_ms=duration_ms3,\n",
    "    metadata={\"method\": \"trace_from_result\"},\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Traced: {query3}\")\n",
    "print(f\"   Output: {result3['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare: With Tools vs Without Tools\n",
    "\n",
    "Let's demonstrate the difference in evaluation modes when tools are used vs not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chain WITHOUT tools for comparison\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions to the best of your knowledge.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "simple_chain = simple_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Ask same question to both\n",
    "test_query = \"What is the capital of France and what's 25 * 4?\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Agent with Tools vs Simple Chain\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with tools (Agentic mode)\n",
    "print(\"\\nğŸ”§ AGENT WITH TOOLS (Agentic Mode)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "start_time = time.time()\n",
    "agent_result = agent_executor.invoke({\"input\": test_query})\n",
    "agent_duration = (time.time() - start_time) * 1000\n",
    "\n",
    "# Extract tool calls\n",
    "agent_tool_calls = []\n",
    "for action, observation in agent_result.get(\"intermediate_steps\", []):\n",
    "    agent_tool_calls.append({\n",
    "        \"name\": action.tool,\n",
    "        \"input\": action.tool_input,\n",
    "        \"output\": observation,\n",
    "    })\n",
    "\n",
    "print(f\"Output: {agent_result['output']}\")\n",
    "print(f\"Tools used: {[tc['name'] for tc in agent_tool_calls]}\")\n",
    "print(f\"Duration: {agent_duration:.0f}ms\")\n",
    "\n",
    "# Trace to CERT\n",
    "client.trace(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input_text=test_query,\n",
    "    output_text=agent_result[\"output\"],\n",
    "    duration_ms=agent_duration,\n",
    "    tool_calls=agent_tool_calls,\n",
    "    metadata={\"comparison\": \"with_tools\"},\n",
    ")\n",
    "print(\"\\nâœ… Traced as AGENTIC mode (full metrics available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without tools (Generation mode)\n",
    "print(\"\\nğŸ“ SIMPLE CHAIN WITHOUT TOOLS (Generation Mode)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "start_time = time.time()\n",
    "chain_result = simple_chain.invoke({\"input\": test_query})\n",
    "chain_duration = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"Output: {chain_result}\")\n",
    "print(f\"Tools used: None\")\n",
    "print(f\"Duration: {chain_duration:.0f}ms\")\n",
    "\n",
    "# Trace to CERT\n",
    "client.trace(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input_text=test_query,\n",
    "    output_text=chain_result,\n",
    "    duration_ms=chain_duration,\n",
    "    # No tool_calls, no context -> Generation mode\n",
    "    metadata={\"comparison\": \"without_tools\"},\n",
    ")\n",
    "print(\"\\nâš ï¸ Traced as GENERATION mode (limited metrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the metric difference\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METRIC AVAILABILITY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric              â”‚ Agentic (tools) â”‚ Generation      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Semantic Similarity â”‚ âœ… Available    â”‚ âŒ No context   â”‚\n",
    "â”‚ NLI Contradiction   â”‚ âœ… Available    â”‚ âŒ No context   â”‚\n",
    "â”‚ Grounding Score     â”‚ âœ… Available    â”‚ âŒ No context   â”‚\n",
    "â”‚ SGI (Source-Ground) â”‚ âœ… Available    â”‚ âŒ Undefined    â”‚\n",
    "â”‚ Tool Integration    â”‚ âœ… Available    â”‚ âŒ No tools     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Instruction Adhere  â”‚ âœ… Available    â”‚ âœ… Available    â”‚\n",
    "â”‚ Self-Consistency    â”‚ âœ… Available    â”‚ âœ… Available    â”‚\n",
    "â”‚ Format Compliance   â”‚ âœ… Available    â”‚ âœ… Available    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Key insight: Without context (from tools or explicit), we cannot\n",
    "measure FAITHFULNESS or GROUNDING - the core reliability metrics.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Stats and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get client statistics\n",
    "stats = client.get_stats()\n",
    "\n",
    "print(\"\\nğŸ“Š CERT CLIENT STATS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Traces sent: {stats['traces_sent']}\")\n",
    "print(f\"Traces failed: {stats['traces_failed']}\")\n",
    "print(f\"Traces queued: {stats['traces_queued']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush and close\n",
    "client.flush()\n",
    "client.close()\n",
    "\n",
    "print(\"\\nâœ… CERT client closed\")\n",
    "print(\"\\nView your traces at: https://dashboard.cert-framework.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Automatic Context Extraction**: The CERT SDK automatically extracts context from tool outputs in agentic mode. You don't need to construct context manually.\n",
    "\n",
    "2. **Use `return_intermediate_steps=True`**: When using LangChain agents, always enable this to capture tool calls.\n",
    "\n",
    "3. **Two Integration Methods**:\n",
    "   - **Callback Handler** (`CERTLangChainHandler`): Automatic, real-time capture\n",
    "   - **Manual Tracing**: More control, post-execution tracing\n",
    "\n",
    "4. **Metric Availability**:\n",
    "   - With tools/context: Full reliability metrics (SGI, grounding, NLI)\n",
    "   - Without: Only quality signals (instruction adherence, format)\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "```python\n",
    "# âœ… Good: Let SDK auto-extract context\n",
    "client.trace(tool_calls=tool_calls, ...)\n",
    "\n",
    "# âœ… Good: Use callback handler\n",
    "agent.invoke({\"input\": query}, config={\"callbacks\": [handler]})\n",
    "\n",
    "# âŒ Bad: Agentic mode without tools or context\n",
    "client.trace(eval_mode=\"agentic\", ...)  # Warning: no context!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
